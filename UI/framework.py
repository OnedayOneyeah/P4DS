import os, re, random, json, shutil
from dotenv import load_dotenv

from openai import OpenAI
from tavily import TavilyClient
from codeinterpreterapi import CodeInterpreterSession, settings

from db import collection_query

# === OpenAI & Tavily ì„¤ì • ===
load_dotenv()
openai_api_key = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=openai_api_key)
tavily_api_key = os.getenv("TAVILY_API_KEY")
tavily_client = TavilyClient(api_key=tavily_api_key)

# === Visualization ì´ˆê¸°í™” (ì˜ˆì‹œ) ===
settings.OPENAI_API_KEY=openai_api_key
settings.MODEL = "gpt-4.1-mini"

# === 10ê°œ í•­ëª©ë³„ ì´ˆê¸° Score/Confidence (None) ===
report_card = {
    "Clarity of Vision":       {"score": None, "confidence": None},
    "Team Competency":         {"score": None, "confidence": None},
    "Customer Understanding":  {"score": None, "confidence": None},
    "Product-Market Fit":      {"score": None, "confidence": None},
    "Traction & KPIs":         {"score": None, "confidence": None},
    "Go-to-Market Strategy":   {"score": None, "confidence": None},
    "Financial Readiness":     {"score": None, "confidence": None}
}
num_criteria = len(report_card)

# í‰ê°€ì§€í‘œë³„ ì •ì˜ -> ì´í›„ í‰ê°€ ì‹œ ê° í‰ê°€ì§€í‘œì™€ ì •ì˜ ì—°ê²°í•´ì„œ ì‚¬ìš©
criterion_definitions = {
    "Clarity of Vision": (
        "The clarity and coherence of the startupâ€™s long-term goals and mission. It assesses whether the startup has a well-defined direction and purpose."
    ),
    "Team Competency": (
        "The skills, experience, and execution capability of the founding and core team. This includes technical proficiency, role distribution, and adaptability."
    ),
    "Customer Understanding": (
        "The degree to which the startup understands its target customersâ€™ needs, pain points, and behaviors, and how that understanding shapes product or service design."
    ),
    "Product-Market Fit": (
        "How well the product or service satisfies an existing market demand. Includes customer validation, engagement, and retention."
    ),
    "Traction & KPI": (
        "The measurable indicators of progress such as user growth, revenue, engagement, or retention. Demonstrates early signs of market validation or momentum."
    ),
    "Go-to-Market Strategy": (
        "The effectiveness and feasibility of the plan for customer acquisition, distribution, and messaging. Includes sales channels, marketing strategy, and outreach mechanisms."
    ),
    "Financial Readiness": (
        "The current financial state and the ability to sustain or scale the business. Includes budgeting, burn rate awareness, and fundraising preparedness."
    ),
}

CONFIDENCE_THRESHOLD = 80

# ìŠ¤íƒ€íŠ¸ì—… stageì— ë”°ë¼ í‰ê°€ì§€í‘œë³„ ê°€ì¤‘ì¹˜ ë¶€ì—¬í•´ overall_score ê³„ì‚°í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ ì •ë³´ ì €ì¥
STAGE_WEIGHTS = {
    "early": {
        "Clarity of Vision": 0.25,
        "Team Competency": 0.25,
        "Customer Understanding": 0.20,
        "Product-Market Fit": 0.15,
        "Traction & KPIs": 0.05,
        "Go-to-Market Strategy": 0.05,
        "Financial Readiness": 0.05
    },
    "growth": {
        "Clarity of Vision": 0.15,
        "Team Competency": 0.15,
        "Customer Understanding": 0.20,
        "Product-Market Fit": 0.20,
        "Traction & KPIs": 0.15,
        "Go-to-Market Strategy": 0.10,
        "Financial Readiness": 0.05
    },
    "scaling": {
        "Clarity of Vision": 0.10,
        "Team Competency": 0.15,
        "Customer Understanding": 0.10,
        "Product-Market Fit": 0.15,
        "Traction & KPIs": 0.20,
        "Go-to-Market Strategy": 0.20,
        "Financial Readiness": 0.10
    }
}

def all_criteria_above_threshold(report: dict, threshold: int) -> bool:
    """
    ëª¨ë“  í•­ëª©ì˜ confidenceê°€ threshold ì´ìƒì¸ì§€ ì²´í¬.
    Noneì´ë©´ thresholdë¥¼ ë‹¬ì„±í–ˆë‹¤ê³  ë³¼ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ False.
    """
    for _, v in report.items():
        if v['confidence'] is None or v['confidence'] < threshold:
            return False
    return True

def print_report(report: dict):
    """
    report_cardë¥¼ ì‚¬ëŒì´ ì½ê¸° ì¢‹ê²Œ í”„ë¦°íŠ¸.
    Noneì´ë©´ 'N/A'ë¡œ í‘œì‹œ
    """
    print("\n===== í˜„ì¬ ìŠ¤íƒ€íŠ¸ì—… ì§„ë‹¨ ë³´ê³ ì„œ =====")
    i = 1
    for criteria, data in report.items():
        score_str = data['score'] if data['score'] is not None else "N/A"
        conf_str = data['confidence'] if data['confidence'] is not None else "N/A"
        print(f"{i}. {criteria}: {score_str} / 5ì  (Confidence: {conf_str}%)")
        i += 1
    print("==================================\n")

def llm_call(system_prompt: str, user_prompt: str, temperature: float = 0.7) -> str:
    """
    OpenAI APIë¥¼ í˜¸ì¶œí•´ system+user í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ë‹µë³€ì„ ìƒì„±.
    """
    completion = client.chat.completions.create(
        model="gpt-4.1-mini",  # ëª¨ë¸ëª…ì€ ì˜ˆì‹œ (ì ì ˆíˆ êµì²´ ê°€ëŠ¥)
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        temperature=temperature
    )
    return completion.choices[0].message.content.strip()

# ëª¨ë“  user input ëª¨ì•„ì„œ ìŠ¤íƒ€íŠ¸ì—…ì˜ ë‹¨ê³„ LLMìœ¼ë¡œë¶€í„° ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜ (generate_business_reportì—ì„œ ì‚¬ìš©)
def infer_startup_stage_from_input(user_text: str) -> str:
    system_prompt = (
        "You are a startup evaluator AI.\n"
        "Classify the startup's stage based ONLY on the description below.\n\n"
        "Possible stages:\n"
        "- [early]: Idea stage or pre-revenue\n"
        "- [growth]: Has launched product, early user or revenue growth\n"
        "- [scaling]: Proven product-market fit and scaling business\n\n"
        "Return ONLY the stage name wrapped in brackets, e.g., [growth].\n"
        "Do NOT add explanation, just return the label."
    )
    user_prompt = f"Startup description:\n{user_text}\n\nStartup Stage:"
    raw_response = llm_call(system_prompt, user_prompt, temperature=0.0)

    # ì •ê·œí‘œí˜„ì‹ ê¸°ë°˜ íŒŒì‹±
    match = re.search(r"\[(early|growth|scaling)\]", raw_response.lower())
    if match:
        return match.group(1)
    return "unknown"

# ìŠ¤íƒ€íŠ¸ì—… stage ê³ ë ¤í•´ì„œ ì§€í‘œë³„ ê°€ì¤‘ì¹˜ ë¶€ì—¬í•´ overall score ê³„ì‚°
def compute_weighted_overall_score(report: dict, stage: str) -> float:
    weights = STAGE_WEIGHTS.get(stage, {})
    total = 0.0
    for criterion, data in report.items():
        score = data.get("score")
        weight = weights.get(criterion, 0)
        if score is not None:
            total += score * weight
    return round(total, 2)

# ìŠ¤íƒ€íŠ¸ì—… stage ê³ ë ¤í•´ì„œ ë¶€ì¡±í•œ ë¶€ë¶„ì— ëŒ€í•œ í”¼ë“œë°± ë°›ëŠ” ë¶€ë¶„
def generate_stage_feedback_text(report: dict, stage: str) -> str:
    """
    í˜„ì¬ ìŠ¤íƒ€íŠ¸ì—…ì˜ í‰ê°€ ì ìˆ˜ì™€ ë‹¨ê³„(stage)ë¥¼ ë°”íƒ•ìœ¼ë¡œ,
    LLMì´ ë¶€ì¡±í•œ í•­ëª©ê³¼ ê·¸ ì´ìœ ë¥¼ ìš”ì•½ í”¼ë“œë°±ìœ¼ë¡œ ìƒì„±.

    ì¶œë ¥ì€ ìì—°ì–´ bullet-point í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ë¨.
    """

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ì—­í• ê³¼ ëª©ì  ì •ì˜
    system_prompt = (
        "You are a startup evaluation assistant.\n"
        "Given a startup's current stage and evaluation scores, "
        "your job is to identify which criteria are relatively weak or risky at this stage.\n\n"
        "Explain briefly and clearly why each low-scoring criterion matters at this stage.\n"
        "Return your feedback in 2~5 bullet points in plain English (no JSON)."
    )

    # í‰ê°€ì§€í‘œ ìš”ì•½ í…ìŠ¤íŠ¸ êµ¬ì„±
    def score_str(k, v):
        return f"{k}: Score = {v['score']}/5, Confidence={v['confidence']}%"

    report_summary = "\n".join([
        score_str(k, v) for k, v in report.items()
        if v["score"] is not None and v["confidence"] is not None
    ])

    # ì‚¬ìš©ì í”„ë¡¬í”„íŠ¸: ì ìˆ˜ + ë‹¨ê³„ ì „ë‹¬
    user_prompt = (
        f"Startup Stage: {stage}\n\n"
        f"Evaluation Scores:\n{report_summary}\n\n"
        "Based on this information, which areas should the startup improve given its current stage?"
    )

    # LLM í˜¸ì¶œ
    feedback_text = llm_call(system_prompt, user_prompt, temperature=0.7)

    return feedback_text.strip()

def parse_report_card_json(json_str: str) -> dict:
    """
    LLMì´ ì¤€ JSONì„ íŒŒì‹±í•´, 10ê°œ í‚¤ê°€ ëª¨ë‘ ìˆëŠ”ì§€, ê° valueì— "score","confidence"ê°€ ìˆëŠ”ì§€ ê²€ì‚¬.
    """
    try:
        data = json.loads(json_str)
    except json.JSONDecodeError:
        return None

    if not isinstance(data, dict):
        return None

    required_keys = list(report_card.keys())
    if len(data.keys()) != num_criteria:
        return None
    for k in required_keys:
        if k not in data:
            return None
        if not isinstance(data[k], dict):
            return None
        if "score" not in data[k] or "confidence" not in data[k]:
            return None

    return data

def update_report_card(report: dict, new_data: dict):
    """
    report_cardì˜ score/confidenceë¥¼ new_dataë¡œ ê°±ì‹ 
    """
    for criterion in report.keys():
        report[criterion]["score"] = new_data[criterion]["score"]
        report[criterion]["confidence"] = new_data[criterion]["confidence"]

def search_internet(query: str) -> str:
    response = tavily_client.search(query)
    if response:
        return response
    else:
        return f"[Internet] '{query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."

def search_db(query: str, db_type: str) -> str:
    """
    ChromaDBì—ì„œ query_texts=[query]ë¡œ ê²€ìƒ‰
    db_typeì— ë§ì¶”ì–´ ê²€ìƒ‰
    """  
    results = collection_query(query_texts=[query], n_results=3, db_type=db_type)
    docs = results.get("documents", [[]])[0]
    if docs:
        joined_docs = "\n".join([f"- {d}" for d in docs])
        return f"[DB ê²€ìƒ‰ ê²°ê³¼ - {db_type}]\n{joined_docs}"
    else:
        return f"[DB ê²€ìƒ‰ ê²°ê³¼ - {db_type}] '{query}' ê´€ë ¨ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤."

def refine_criterion_output(criterion: str, all_context: str) -> str:
    """
    'criterion' í•­ëª©ì— ëŒ€í•´ ë³´ì™„í•  ì ì„ ë¨¼ì € ì¶”ì¶œí•œ ë’¤,
    ê·¸ ë³´ì™„ì ì„ ë°˜ì˜í•˜ì—¬ ë‹¤ì‹œ ì‘ì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ìµœì¢… ë°˜í™˜í•œë‹¤.
    """

    # 1) LLMìœ¼ë¡œë¶€í„° ê°œì„ (ë³´ì™„) í¬ì¸íŠ¸ë¥¼ ë¨¼ì € ë°›ì•„ì˜¨ë‹¤.
    system_prompt_1 = (
        "You are an AI assistant analyzing a specific section of a startup business report.\n"
        "Your task is to identify any weaknesses or areas for improvement in the text related to this criterion.\n"
        "List them clearly so we can address them in the next step.\n\n"
        "Return these suggested improvements in plain text (e.g., bullet points)."
    )
    user_prompt_1 = (
        f"Criterion to refine: '{criterion}'\n\n"
        f"Here is all the context collected so far:\n{all_context}\n\n"
        "Please list the points or areas that should be improved, clarified, or expanded upon for this criterion."
    )
    improvement_points = llm_call(system_prompt_1, user_prompt_1, temperature=0.7)

    # 2) LLMì—ê²Œ, ìœ„ì—ì„œ ë°›ì€ ê°œì„  í¬ì¸íŠ¸ë¥¼ ë°˜ì˜í•´ ë” ê¹Šê³  êµ¬ì²´ì ì¸ í…ìŠ¤íŠ¸ë¡œ ë‹¤ì‹œ ì‘ì„±í•´ë‹¬ë¼ê³  ìš”ì²­í•œë‹¤.
    system_prompt_2 = (
        "You are an AI assistant refining a specific section of a startup business report.\n"
        "You have a list of improvements to address.\n"
        "Use them to produce a revised, more detailed discussion for this criterion, "
        "providing clarity, depth, and actionable insights.\n\n"
        "Return the refined explanation in plain text (no JSON)."
    )
    user_prompt_2 = (
        f"Criterion to refine: '{criterion}'\n\n"
        f"Improvement points:\n{improvement_points}\n\n"
        f"Here is the context again:\n{all_context}\n\n"
        "Incorporate the listed improvements into the final refined text."
    )
    refined_text = llm_call(system_prompt_2, user_prompt_2, temperature=0.7)

    return refined_text

def analyze_and_visualize(all_context: str) -> dict:
    """
    CodeInterpreterë¡œ í•˜ì—¬ê¸ˆ 3C ê´€ì  (e.g., Company, Customer and Competitors)ì—ì„œ ì‹œì¥ì„ ë¶„ì„í•˜ê³ , ì´ë¥¼ ì‹œê°í™” í•˜ë„ë¡ í•¨.
    """

    # 1) LLMìœ¼ë¡œë¶€í„° visualizationì„ ìœ„í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ë°›ì•„ì˜¤ê¸°
    system_prompt_1 = (
    "You are an AI assistant tasked with supporting the creation of a business report.\n"
    "Your focus is on evaluating the market perspective within the 3C framework (Customer, Competitor, Company).\n"
    "Your goal is to identify relevant data points that should be explored and recommend prompts for further analysis and data visualization.\n\n"
    "Structure your response using the following format:\n"
    "[Task] A brief description of the specific analysis or insight to be developed.\n\n"
    "[Data] A bullet-point list of the key data elements required to support the analysis.\n\n"
    "[Chart Specs] A bullet-point list of suggested visualizations, including the type of chart and what variables to compare."
    )
    # all_context = "AI Healthcare startup in the U.S.A. In series A, the item is analzing users' biometrics using medical mobiel devices"
    user_prompt_1 = (
    f"Here is the current context and information gathered about a startup company:\n{all_context}\n\n"
    "Based on this context, please identify areas that need to be improved, clarified, or expanded in order to strengthen the market analysis under the 3C framework."
    )
    user_input = llm_call(system_prompt_1, user_prompt_1, temperature=0.7)
    user_input += "\n[Output] A highâ€‘resolution PNG (â‰ˆâ€¯1920Ã—1080) suitable for presentations."

    PROMPT_1 = (
        "You are a highly skilled data analyst assigned to address the user's query using reliable, authoritative, and verifiable data sources.\n\n"
        "Your objectives:\n"
        "- Acquire accurate, relevant, and reputable datasets that directly support the userâ€™s inquiry.\n"
        "- Structure the data in a clean, well-formatted pandas DataFrame with appropriate data types and labels.\n"
        "- Conduct a meaningful, insight-driven analysis that provides direct, data-backed answers to the userâ€™s question.\n"
        "- Create compelling, well-labeled visualizations using matplotlib to effectively communicate your findings.\n\n"
        "Visualization Requirements:\n"
        "- Every chart must include:\n"
        "  â€¢ A clear and descriptive title.\n"
        "  â€¢ Properly labeled axes and legends, where applicable.\n"
        "  â€¢ Exact names of companies, institutions, or reports referenced in the data.\n"
        "  â€¢ Visible citation of the data source with URL.(e.g., in a corner of the plot).\n\n"
        "Analysis Explanation:\n"
        "- Write a concise and insightful narrative summarizing:\n"
        "  â€¢ The methodology and key findings.\n"
        "  â€¢ Any observed trends, comparisons, or outliers.\n"
        "  â€¢ The broader implications of the analysis.\n"
        "- Provide full URLs to all data sources used in your analysis to ensure traceability and transparency.\n\n"
        "Important Notes:\n"
        "- Only use data from trustworthy, recognized sources (e.g., official government portals, academic institutions, reputable organizations).\n"
        "- Ensure the analysis is self-contained, reproducible, and directly aligned with the userâ€™s request.\n\n"
        "User Query: {user_input}\n"
    )

    # # (2) Tavily search APIë¥¼ í†µí•´ì„œ ì´ë¯¸ retrievalí•œ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì´ë¥¼ ì‹œê°í™” í•˜ëŠ” ê²½ìš°
    # PROMPT_2 = (
    # "You are a data analyst. Based on the dataset below, analyze the topic '{query}' and create a clear visualization. \n"
    # "\t- The dataset is in pandas DataFrame format and contains the following: {response} \n"
    # "\t- Provide a concise explanation of your analysis. \n"
    # "\t- Generate the visualization using matplotlib. \n"
    # "\t- Make sure to include font settings in the code to properly display Korean characters (e.g., use 'Malgun Gothic' on Windows or 'AppleGothic' on macOS). \n"
    # "\t- Also include the setting to prevent minus signs from breaking when rendering Korean text in matplotlib. \n"
    # )

    with CodeInterpreterSession() as session:
        # generate a response based on user input
        response = session.generate_response(
            PROMPT_1.format(user_input = user_input)
        )

    # save visualized images
    response_txt = response.content  # collected_contextsì— ì¶”ê°€ ê°€ëŠ¥

    SOURCE_CODEBOX_DIR = ".codebox"

    # ëŒ€ìƒ í´ë” ì„¤ì •: UI/static/images
    # app.pyê°€ UI í´ë”ì— ìˆìœ¼ë¯€ë¡œ, ìƒëŒ€ ê²½ë¡œëŠ” "static/images"ê°€ ë©ë‹ˆë‹¤.
    DEST_FIGURES_DIR = os.path.join("static", "images") 
    os.makedirs(DEST_FIGURES_DIR, exist_ok=True)

    if os.path.exists(SOURCE_CODEBOX_DIR) and os.path.isdir(SOURCE_CODEBOX_DIR):
        print(f"'{SOURCE_CODEBOX_DIR}' í´ë”ì—ì„œ íŒŒì¼ ë³µì‚¬ ì‹œë„ -> '{DEST_FIGURES_DIR}'") # ë¡œê·¸ ë³€ê²½
        copied_files_count = 0
        for filename in os.listdir(SOURCE_CODEBOX_DIR):
            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif')):
                source_file_path = os.path.join(SOURCE_CODEBOX_DIR, filename)
                destination_file_path = os.path.join(DEST_FIGURES_DIR, filename)
                try:
                    shutil.copy2(source_file_path, destination_file_path)
                    print(f"íŒŒì¼ ë³µì‚¬ë¨: {source_file_path} -> {destination_file_path}")
                    copied_files_count += 1
                except Exception as e:
                    print(f"íŒŒì¼ ë³µì‚¬ ì¤‘ ì˜¤ë¥˜ ({source_file_path}): {e}")
        if copied_files_count == 0:
            print(f"'{SOURCE_CODEBOX_DIR}' í´ë”ì—ì„œ ë³µì‚¬í•  ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"ì†ŒìŠ¤ í´ë” '{SOURCE_CODEBOX_DIR}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CodeInterpreterê°€ íŒŒì¼ì„ ìƒì„±í•˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        
    return response_txt

def generate_db_query(criterion: str, all_context: str, db_type: str) -> str:
    """
    LLMì—ê²Œ í•´ë‹¹ criterionê³¼ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ìœ¼ë¡œ DBì— ì‚¬ìš©í•  ì ì ˆí•œ ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ê²Œ í•¨.
    """
    db_desc = "real-world Korean startup company examples" if db_type == "startup" else "AI technology trends and statistics"

    system_prompt = (
        "You are an AI assistant generating database search queries for a business report.\n"
        f"The database is focused on {db_desc}.\n"
        "Given the criterion and the context so far, generate a specific search query that will help retrieve relevant documents.\n"
        "Only return the search query. No explanations, no JSON."
    )

    user_prompt = (
        f"Criterion: {criterion}\n\n"
        f"Context:\n{all_context}\n\n"
        "Search query:"
    )

    query = llm_call(system_prompt, user_prompt, temperature=0.3)
    return query.strip()

def generate_user_question_for_criterion(criterion: str, all_context: str) -> str:
    """
    LLMì—ê²Œ:
      'í•´ë‹¹ criterionì„ ê°œì„ í•˜ê¸° ìœ„í•´ ì‚¬ìš©ìì—ê²Œ ì–´ë–¤ ì„¸ë¶€ ì •ë³´ë¥¼ ë¬¼ì–´ë´ì•¼ í•˜ëŠ”ì§€'
    ë¥¼ ë¬»ëŠ”ë‹¤. LLMì´ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ë¬¸ì¥ì„ ë°˜í™˜.
    """
    system_prompt = (
        "You are an AI assistant helping gather specific user input. "
        "Given the context and the chosen criterion, generate a short list of specific questions "
        "the user should answer in detail. The output should be plain text (no JSON)."
    )
    user_prompt = (
        f"Criterion of focus: '{criterion}'\n\n"
        f"Context so far:\n{all_context}\n\n"
        "Based on what is missing or uncertain for this criterion, "
        "create a short set of bullet-point questions for the user to answer. "
        "Be as concrete as possible."
    )
    raw_question_text = llm_call(system_prompt, user_prompt, temperature=0.7)

    # LLM ì‘ë‹µ íŒŒì‹±í•˜ì—¬ ë¦¬ìŠ¤íŠ¸ ìƒì„±
    questions_list = []
    # ê° ì¤„ì„ í™•ì¸í•˜ì—¬ ì§ˆë¬¸ íŒ¨í„´ (-, ìˆ«ì., *)ìœ¼ë¡œ ì‹œì‘í•˜ëŠ”ì§€ ê²€ì‚¬
    lines = raw_question_text.strip().split('\n')
    for line in lines:
        stripped_line = line.strip()
        # ë¶ˆë › í¬ì¸íŠ¸ë‚˜ ë²ˆí˜¸ ë§¤ê¸°ê¸° ì œê±° í›„ ë‚´ìš© ì¶”ì¶œ
        # ì˜ˆ: "- ì§ˆë¬¸ ë‚´ìš©", "1. ì§ˆë¬¸ ë‚´ìš©", "* ì§ˆë¬¸ ë‚´ìš©" ë“± ì²˜ë¦¬
        match = re.match(r'^[\-\*\d]+\.?\s*(.*)', stripped_line)
        if match:
            question_content = match.group(1).strip()
            if question_content: # ë‚´ìš©ì´ ìˆëŠ” ê²½ìš°ë§Œ ì¶”ê°€
                questions_list.append(question_content)
        elif stripped_line: # íŒ¨í„´ì´ ì—†ì§€ë§Œ ë‚´ìš©ì´ ìˆëŠ” ì¤„ë„ ì¼ë‹¨ ì¶”ê°€ (LLMì´ í˜•ì‹ì„ ì•ˆ ì§€í‚¬ ê²½ìš° ëŒ€ë¹„)
            questions_list.append(stripped_line)

    # print(f"ìƒì„±ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ({len(questions_list)}ê°œ): {questions_list}")
    return questions_list

def generate_internet_search_query(criterion: str, all_context: str) -> str:
    """
    LLMì—ê²Œ: 'í•´ë‹¹ criterion ê´€ë ¨í•´ì„œ ì¸í„°ë„·ì—ì„œ ì–´ë–¤ í‚¤ì›Œë“œë¥¼ ê²€ìƒ‰í•´ì•¼
    í•„ìš”í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€'ë¥¼ ë¬¼ì–´ë´„.
    """
    system_prompt = (
        "You are an AI assistant that decides the best internet search query "
        "to gather more information about a certain criterion in a startup business report.\n"
        "Return ONLY the recommended search query in plain text (no JSON)."
    )
    user_prompt = (
        f"Criterion of focus: '{criterion}'\n\n"
        f"Context so far:\n{all_context}\n\n"
        "Based on what's missing or uncertain for this criterion, propose a concise search query "
        "that would help gather the most relevant insights or data from the internet."
    )
    suggested_query = llm_call(system_prompt, user_prompt, temperature=0.7)
    return suggested_query.strip()

def perform_action(action: str, target_criteria: str, collected_contexts: list) -> str:
    """
    LLMì´ ê²°ì •í•œ actionì„ ì‹¤ì œ ìˆ˜í–‰í•˜ì—¬ ê²°ê³¼ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.
    ê²°ê³¼ í…ìŠ¤íŠ¸ëŠ” ê·¸ëŒ€ë¡œ collected_contextsì— ì¶”ê°€ë˜ì–´
    ì´í›„ ë³´ê³ ì„œ ì—…ë°ì´íŠ¸/ë¶„ì„ì— í™œìš©ëœë‹¤.

    * ì—¬ê¸°ì„œ ë°˜í™˜ ë¬¸ìì—´ ì•ì— ì‹ë³„ìë¥¼ ë¶™ì—¬ì¤Œìœ¼ë¡œì¨,
      ë‚˜ì¤‘ì— generate_business_reportì—ì„œ êµ¬ì¡°ì ìœ¼ë¡œ í™œìš© ê°€ëŠ¥í•˜ë„ë¡ í•¨.
    """
    if action == "AskUser":
        # 1) LLMì—ê²Œ 'ë¬´ì—‡ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¬¼ì–´ë´ì•¼ í•˜ëŠ”ê°€'ë¥¼ ìš”ì²­
        full_context_str = "\n".join(collected_contexts)
        question_prompt = generate_user_question_for_criterion(target_criteria, full_context_str)
        return (None, question_prompt)
    
    elif action == "SearchDB_startup":
        full_context_str = "\n".join(collected_contexts)
        query = generate_db_query(target_criteria, full_context_str, db_type="startup")
        db_result = search_db(query, db_type="startup")
        return (f"DB_SUMMARY: {db_result}", None)

    elif action == "SearchDB_report":
        full_context_str = "\n".join(collected_contexts)
        query = generate_db_query(target_criteria, full_context_str, db_type="stanford")
        db_result = search_db(query, db_type="stanford")
        return (f"DB_SUMMARY: {db_result}", None)

    elif action == "SearchInternet":
        # 1) LLMì—ê²Œ 'ì¸í„°ë„·ì—ì„œ ê²€ìƒ‰í•  query'ë¥¼ ìƒì„±í•´ë‹¬ë¼ê³  ìš”ì²­
        full_context_str = "\n".join(collected_contexts)
        suggested_query = generate_internet_search_query(target_criteria, full_context_str)

        # 2) ì‹¤ì œ ì¸í„°ë„· ê²€ìƒ‰ ìˆ˜í–‰
        net_result = search_internet(suggested_query)

        # 3) ê²°ê³¼ë¥¼ ë°˜í™˜ (ê²€ìƒ‰ì–´ + ê²€ìƒ‰ ê²°ê³¼) (ì‹ë³„ì: INTERNET_SUMMARY)
        return (f"INTERNET_SUMMARY: (Internet Search Query: '{suggested_query}')\n{net_result}", None)

    elif action == "RefineOutput":
        # RefineOutput ì‹œ, LLM ì¶”ê°€ í˜¸ì¶œ
        full_context_str = "\n".join(collected_contexts)
        refined_text = refine_criterion_output(target_criteria, full_context_str)
        # ì‹ë³„ì: REFINED_OUTPUT
        return (f"REFINED_OUTPUT: (Refined Output about {target_criteria})\n{refined_text}", None)

    elif action == "AnalyzeAndVisualize":
        full_context_str = "\n".join(collected_contexts)
        analysis_with_figure = analyze_and_visualize(full_context_str)
        # ì´ì œ figure_pathë„ í•¨ê»˜ ë°˜í™˜
        return (
            f"3C_ANALYSIS: {analysis_with_figure}",
            None
        )

    elif action == "NoActionNeeded":
        return ("(No further actions required.)", None)
    else:
        return (f"(Unknown Action: {action})", None)
 
def ask_llm_for_next_action(report: dict, collected_texts: list, action_history: list) -> dict:
    """
    LLMì—ê²Œ â€œë‹¤ìŒ ì•¡ì…˜â€ + â€œì–´ëŠ í•­ëª©(criterion)ì¸ì§€â€ + "ì™œ ê·¸ ì•¡ì…˜ì„ ê³¨ëëŠ”ì§€(rationale)"ë¥¼
    JSON í˜•ì‹ìœ¼ë¡œ ë°›ëŠ”ë‹¤.
    """
    system_prompt = (
        "You are an AI assistant finalizing a startup's business report.\n\n"
        "You have 7 possible actions:\n"
        " 1) AskUser       : Need more specific details from user\n"
        " 2) SearchDB_startup     : Need real-world Korean startup company examples\n"
        " 3) SearchDB_report    : Need AI technology trend, statistics information\n"
        " 4) SearchInternet: Need external info from the web\n"
        " 5) RefineOutput  : Have enough info, want to refine/improve writing\n"
        " 6) AnalyzeAndVisualize : Perform data analysis and generate visualizations using retrieved user and market information.\n"
        " 7) NoActionNeeded: Everything is sufficiently addressed\n\n"
        "When deciding, consider any info gaps or low confidence in the 7 criteria.\n"
        "Also, review the action history and **choose actions that have not yet been tried**.\n" # ìˆ˜ì •
        "Avoid repeating the same actions. Especially, if 'AskUser' action was perfrmed before, do not chose 'AskUser' action.\n\n" # ìˆ˜ì •
        "Return your decision in JSON with EXACTLY these three keys:\n"
        "  \"criterion\"  -> one of the 10 criteria, or \"None\" if no focus\n"
        "  \"action\"     -> one of [AskUser, SearchDB_startup, SearchDB_report, SearchInternet, RefineOutput, AnalyzeAndVisualize, NoActionNeeded]\n"
        # "  \"action\"     -> one of [SearchDB_startup, SearchDB_report, SearchInternet, AnalyzeAndVisualize, NoActionNeeded]\n"
        "  \"rationale\"  -> a short sentence explaining why you chose this action.\n\n"
        "No extra keys, no disclaimers, no additional text. ONLY JSON."
    )

    def sc_str(d):
        return f"Score={d['score'] if d['score'] is not None else 'N/A'}, Confidence={d['confidence'] if d['confidence'] is not None else 'N/A'}%"

    report_summary = "\n".join([
        f"{k}: {sc_str(v)}"
        for k, v in report.items()
    ])

    accumulated_context = "\n---\n".join(collected_texts)

    # ì•¡ì…˜ íˆìŠ¤í† ë¦¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ í•©ì¹¨
    action_history_text = "\n".join([
        f"[{i+1}] {entry}"
        for i, entry in enumerate(action_history)
    ])

    print("===========", action_history_text)

    user_prompt = (
        f"Current report state:\n{report_summary}\n\n"
        f"Action history so far:\n{action_history_text}\n\n"
        f"Collected contexts:\n{accumulated_context}\n\n"
        "Which single criterion is the biggest priority now, and which action is most appropriate?\n"
        "Also provide a short rationale explaining your choice.\n"
        "Important: Output EXACTLY and ONLY JSON in the following format:\n\n"
        "{\n"
        "  \"criterion\": \"<one_of_the_10_criteria_or_None>\",\n"
        # "  \"action\": \"<SearchDB_startup_or_SearchDB_report_or_SearchInternet_or_AnalyzeAndVisualize_or_NoActionNeeded>\",\n"
        "  \"action\": \"<AskUser_or_SearchDB_startup_or_SearchDB_report_or_SearchInternet_or_RefineOutput_or_AnalyzeAndVisualize_or_NoActionNeeded>\",\n"
        "  \"rationale\": \"<short_reason>\"\n"
        "}\n"
    )

    max_tries = 3
    for attempt in range(max_tries):
        raw = llm_call(system_prompt, user_prompt, temperature=0.0)
        print("[ask_llm_for_next_action] Raw LLM Output:\n", raw)  # ë””ë²„ê·¸ ì¶œë ¥

        try:
            action_data = json.loads(raw.strip())
            print("=========")
            print(action_data)
            print("=========")
            # JSON í‚¤ ê²€ì‚¬
            if ("criterion" in action_data) and ("action" in action_data) and ("rationale" in action_data):
                valid_actions = ["AskUser", "SearchDB_startup", "SearchDB_report", "SearchInternet", "RefineOutput", "AnalyzeAndVisualize", "NoActionNeeded"]
                # valid_actions = ["SearchDB_startup", "SearchDB_report", "SearchInternet", "RefineOutput", "AnalyzeAndVisualize", "NoActionNeeded"]
                if action_data["action"] in valid_actions:
                    # # TODO í•˜ë‚˜ì”© ì‹œë„
                    # action_data["action"]="AnalyzeAndVisualize"
                    return action_data
        except Exception:
            pass
 
        print(f"âš ï¸ ì•¡ì…˜ JSON í˜•ì‹ ì˜¤ë¥˜(ì‹œë„ {attempt+1}/{max_tries}), ì¬ì‹œë„í•©ë‹ˆë‹¤...")

    return None

def generate_business_report(report: dict, collected_texts: list) -> str:
    """
    ë‹¨ê³„ ìš”ì•½:
      1) ì „ì²´ ë§¥ë½(DB, ì¸í„°ë„·, ìœ ì € ì…ë ¥ ë“±)ì„ í™œìš©í•´ 'ë³´ê³ ì„œ í…ìŠ¤íŠ¸(ì„¤ëª… ë¶€ë¶„)'ë¥¼ ìƒì„±
      2) ìµœì¢… ì ìˆ˜(Score)/ì‹ ë¢°ë„(Confidence)ëŠ” ì˜¤ì§ user inputë§Œ ê·¼ê±°í•˜ì—¬ ì‚°ì¶œ
      3) ë§ˆí¬ë‹¤ìš´ í˜•ì‹ìœ¼ë¡œ ìµœì¢… ë³´ê³ ì„œ ì‘ì„±

    í•µì‹¬:
      - ë³´ê³ ì„œ í…ìŠ¤íŠ¸(ì„¤ëª…)ëŠ” DBë‚˜ ì¸í„°ë„· ìš”ì•½ë„ ì°¸ê³ í•´ ì¢€ ë” í’ë¶€í•˜ê²Œ ì‘ì„±í•œë‹¤.
      - í•˜ì§€ë§Œ, 10ê°œ ê¸°ì¤€ë³„ ì ìˆ˜ëŠ” "user input"ë§Œ ê·¼ê±°ë¡œ í•œë‹¤.
    """

    # -- (1) collected_textsì—ì„œ ì‹ë³„ìë¡œ ë¶„ë¥˜ --
    user_inputs = []
    db_summaries = []
    net_summaries = []
    refined_outputs = []
    analysis_texts_from_visualization = []
    general_contexts = []

    for c in collected_texts:
        if c.startswith("USER_INPUT:"):
            user_inputs.append(c[len("USER_INPUT:"):].strip())
        elif c.startswith("DB_SUMMARY:"):
            db_summaries.append(c[len("DB_SUMMARY:"):].strip())
        elif c.startswith("INTERNET_SUMMARY:"):
            net_summaries.append(c[len("INTERNET_SUMMARY:"):].strip())
        elif c.startswith("REFINED_OUTPUT:"):
            refined_outputs.append(c[len("REFINED_OUTPUT:"):].strip())
        elif c.startswith("3C_ANALYSIS:"):
            analysis_texts_from_visualization.append(c[len("3C_ANALYSIS_TEXT:"):].strip())
        else: general_contexts.append(c)

    # ì°¸ê³ ìš©ìœ¼ë¡œ êµ¬ì¡°í™”ëœ ì „ì²´ ë§¥ë½(ì„¤ëª…ìš©)
    structured_context = (
        f"**User Input**:\n{''.join(user_inputs)}\n\n"
        f"**DB Summaries**:\n{''.join(db_summaries)}\n\n"
        f"**Internet Summaries**:\n{''.join(net_summaries)}\n\n"
        f"**Refined Outputs**:\n{''.join(refined_outputs)}\n\n"
        f"**3C Analysis (from Visualization step)**:\n{''.join(analysis_texts_from_visualization)}\n\n"
        f"**Other Contexts**:\n{''.join(general_contexts)}"
    )

    # report_card ìš”ì•½ ë¬¸ìì—´ (ë””ìŠ¤í”Œë ˆì´ìš©)
    def sc_str(d):
        s = d['score'] if d['score'] is not None else 'N/A'
        c = d['confidence'] if d['confidence'] is not None else 'N/A'
        return f"Score={s}, Confidence={c}%"

    report_summary = "\n".join([
        f"{k}: {sc_str(v)}"
        for k, v in report.items()
    ])

    # ------------------------------------------------------
    # (1) ë³´ê³ ì„œ ì„¤ëª… í…ìŠ¤íŠ¸ ìƒì„± (DB/ì¸í„°ë„·ë„ ì°¸ê³ )
    # ------------------------------------------------------
    system_prompt_1 = (
        "You are an AI assistant that creates business reports for startups.\n"
        "You have access to user input, as well as references from DB and the internet.\n"
        "Use ALL of that context to refine or improve the textual explanation of the report.\n"
        "However, do NOT provide any new scores or confidences here.\n"
        "Just generate the improved discussion/explanation in plain text."
    )
    user_prompt_1 = (
        f"Current report state:\n{report_summary}\n\n"
        "Below is the structured context collected so far:\n"
        f"{structured_context}\n\n"
        "Please provide an updated, more detailed explanation of the business report, "
        "incorporating any relevant insights from the references."
    )
    refined_report_text = llm_call(system_prompt_1, user_prompt_1, temperature=0.7)

    # ------------------------------------------------------
    # (2) ì ìˆ˜(Score)/ì‹ ë¢°ë„(Confidence) ì‚°ì¶œ (ì˜¤ì§ user inputë§Œ ì‚¬ìš©)
    # ------------------------------------------------------
    # user inputë“¤ì„ í•˜ë‚˜ë¡œ í•©ì¹¨
    user_only_input_text = "\n".join(user_inputs).strip()
    if not user_only_input_text:
        user_only_input_text = "(No user input provided.)"

    # ëˆ„ì ëœ user input ê¸°ë°˜ìœ¼ë¡œ ìŠ¤íƒ€íŠ¸ì—… ë‹¨ê³„ ì¶”ì¸¡
    startup_stage = infer_startup_stage_from_input(user_only_input_text)

    # report_cardì— ì €ì¥ëœ ê° í‰ê°€ì§€í‘œë¥¼ ëŒë©° ì§€í‘œë³„ ì •ì˜ë¥¼ criterion_definitionsì™€ ë¶™ì—¬ì£¼ê¸°
    criteria_description_block = "\n".join([
    f"{i+1}) \"{k}\": {criterion_definitions.get(k, '')}" for i, k in enumerate(report_card.keys())
    ])
    
    # ì›ë˜ ì•„ë˜ í”„ë¡¬í”„íŠ¸ ë‚´ì— í‰ê°€ì§€í‘œ ì´ë¦„ ë‚˜ì—´ë˜ì–´ ìˆì—ˆëŠ”ë° ìœ„ blockìœ¼ë¡œ ì±„ìš°ëŠ” ê±¸ë¡œ ëŒ€ì²´
    system_prompt_2 = (
        "You are an AI assistant that updates the score and confidence of EXACTLY these 10 criteria:\n"
        f"{criteria_description_block}\n\n"
        "IMPORTANT: For scoring and confidence, you must rely ONLY on the user's input below.\n"
        "Ignore any DB or internet data for the actual scoring.\n\n"
        "You MUST ONLY output valid JSON with these EXACT 7 keys. No more, no less, no renaming.\n"
        "Each key => {\"score\": (1~5), \"confidence\": (0~100)}. No extra text."
    )
    user_prompt_2 = (
        "Below is the user's input (the only source for your scoring):\n"
        f"{user_only_input_text}\n\n"
        "Now output ONLY JSON for the updated score/confidence. "
        "Use exactly the 10 keys listed. No extra keys or text."
    )

    new_report_data = None
    max_tries = 3
    for attempt in range(max_tries):
        raw_json_output = llm_call(system_prompt_2, user_prompt_2, temperature=0.0)
        print("[generate_business_report] Raw JSON from LLM:\n", raw_json_output)  # ë””ë²„ê·¸ ë¡œê·¸

        parsed = parse_report_card_json(raw_json_output)
        if parsed is not None:
            new_report_data = parsed
            break
        else:
            print(f"âš ï¸ JSON í˜•ì‹ ì˜¤ë¥˜(ì‹œë„ {attempt+1}/{max_tries}), ì¬ìš”ì²­í•©ë‹ˆë‹¤...")

    if new_report_data:
        update_report_card(report, new_report_data)
    else:
        print("âŒ 3íšŒ ì‹œë„ í›„ì—ë„ JSON íŒŒì‹± ì‹¤íŒ¨. report_card ì—…ë°ì´íŠ¸ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
    
    # report score ì—…ë°ì´íŠ¸ ì´í›„ overall score ê³„ì‚°
    overall_score = compute_weighted_overall_score(report, startup_stage)

    # ------------------------------------------------------
    # (3) ìµœì¢… 'ë§ˆí¬ë‹¤ìš´' í˜•íƒœì˜ ë³´ê³ ì„œ ìƒì„±
    # ------------------------------------------------------
    system_prompt_3 = (
        "You are an AI assistant creating a final business report in Markdown format.\n"
        "We have 10 criteria, each with an updated Score and Confidence.\n\n"
        "The final report structure should be:\n"
        "# Startup Diagnostic Report\n"
        "## Introduction\n"
        "(A short overview of the startup's current status)\n\n"
        "## 3C Analysis\n"
        "### Company\n"
        "(Team, resources, culture, etc.)\n\n"
        "### Competitors\n"
        "(Competitive landscape)\n\n"
        "### Customers\n"
        "(Target segments, user needs, insights)\n\n"
        "## Criteria Evaluation\n"
        "For each of the 10 criteria, create a subsection:\n"
        "### {Criterion Name}\n"
        "- Score: X/5\n"
        "- Confidence: Y%\n"
        "- Rationale:\n"
        "  (Short explanation)\n\n"
        "## Conclusion\n"
        "(Summarize key findings and next steps)\n\n"
        "Only output valid Markdown."
    )

    updated_report_summary = "\n".join([
        f"{k}: {sc_str(v)}"
        for k, v in report.items()
    ])

    references_md = "## References\n"
    if user_inputs:
        references_md += "\n**User Input**\n"
        for i, ui in enumerate(user_inputs, 1):
            references_md += f"- User Input #{i}: {ui}\n"
    if db_summaries:
        references_md += "\n**DB Summaries (RAG)**\n"
        for i, dbs in enumerate(db_summaries, 1):
            references_md += f"- DB Ref #{i}: {dbs}\n"
    if net_summaries:
        references_md += "\n**Internet Summaries**\n"
        for i, ns in enumerate(net_summaries, 1):
            references_md += f"- Net Ref #{i}: {ns}\n"
    if refined_outputs:
        references_md += "\n**Refined Outputs**\n"
        for i, ro in enumerate(refined_outputs, 1):
            references_md += f"- Refined #{i}: {ro}\n"
    if analysis_texts_from_visualization:
        references_md += "\n**3C Analysis Texts**\n"
        for i, ao in enumerate(analysis_texts_from_visualization, 1): 
            references_md += f"- Analysis Text #{i}: {ao}...\n"

    system_prompt_4 = (
        "You are an AI assistant. You have a preliminary references list from user, DB, and internet.\n"
        "You also have the final business report context.\n"
        "Your task: review all references, and select only those that are directly relevant and supportive of the report.\n"
        "Omit any redundant or off-topic references.\n\n"
        "Output rules:\n"
        "- If the reference includes a URL, use this format:\n"
        "  ğŸ“ [Title](URL)\n"
        "  (1â€“2 line explanation)\n\n"
        "- If the reference does not have a URL, use this format:\n"
        "  ğŸ“‹ **Title (e.g. from user input or DB)**\n"
        "  (1â€“2 line explanation)\n\n"
        "Leave a blank line between references for readability.\n"
        "Only output the cleaned list of selected references."
    )

    user_prompt_4 = (
        f"Final Report (draft):\n{refined_report_text}\n\n"
        f"Full references:\n{references_md}\n\n"
        "Please filter out any references that are not relevant or are repetitive.\n"
        "Return only the references you think are important for understanding or supporting this business report.\n"
    )

    filtered_references = llm_call(system_prompt_4, user_prompt_4, temperature=0.7)

    user_prompt_3 = (
        f"Updated report card:\n{updated_report_summary}\n\n"
        "Refined text:\n"
        f"{refined_report_text}\n\n"
        "Please produce a comprehensive markdown report with the structure above. "
        "Make sure to include the 3C Analysis and the 7 criteria."
    )

    # LLMìœ¼ë¡œ ìµœì¢… report ìƒì„±
    final_markdown_report = llm_call(system_prompt_3, user_prompt_3, temperature=0.7)

    # ìŠ¤íƒ€íŠ¸ì—… stage ê¸°ë°˜ í”¼ë“œë°± ìƒì„± ë° reportì— í”¼ë“œë°± ë‚´ìš© ì¶”ê°€
    stage_feedback_text = generate_stage_feedback_text(report, startup_stage)

    final_markdown_report += "\n\n## Overall Assessment\n"
    final_markdown_report += f"**Startup Stage**: `{startup_stage}`\n"
    final_markdown_report += f"**Weighted Overall Score**: `{overall_score}/5`\n"

    if stage_feedback_text:
        final_markdown_report += "\n**Stage-Based Feedback:**\n"
        final_markdown_report += stage_feedback_text

    final_markdown_report += "\n\n## Relevant References\n"
    final_markdown_report += filtered_references

    return final_markdown_report

def discussion_loop(final_report: str, all_user_inputs: list):
    """
    (ìƒˆë¡œ ì¶”ê°€ë¨)
    ìµœì¢… ë³´ê³ ì„œì™€ ìœ ì € ì…ë ¥ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ììœ ë¡­ê²Œ ëŒ€í™”í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜.
    """
    print("\n=== [Discussion Mode] ===")
    print("ìµœì¢… ë³´ê³ ì„œ ë° ìœ ì € ì…ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ììœ ë¡­ê²Œ ëŒ€í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.\n")

    while True:
        user_ask = input("User: ")
        if user_ask.strip().lower() == "quit":
            print("Discussion ì¢…ë£Œ.")
            break

        # LLM í˜¸ì¶œ (ì˜ˆì‹œ)
        system_prompt = (
            "You are an AI assistant discussing the final startup report.\n"
            "You have the final markdown report and all user inputs.\n"
            "Answer any user questions or discuss further improvements.\n"
        )
        user_prompt = (
            f"Final Report:\n{final_report}\n\n"
            f"All User Inputs:\n{all_user_inputs}\n\n"
            f"User's question:\n{user_ask}"
        )
        answer = llm_call(system_prompt, user_prompt, temperature=0.7)
        print(f"Assistant: {answer}\n")

def main_business_report_loop():
    iteration_count = 0
    max_iterations = 10

    # ëª¨ë“  ì •ë³´ ëˆ„ì  ë¦¬ìŠ¤íŠ¸
    collected_contexts = []
    # ì•¡ì…˜ ì´ë ¥ ì¶”ì  ë¦¬ìŠ¤íŠ¸
    action_history = []

    # 1) ì‚¬ìš©ì ì´ˆê¸° ì •ë³´
    initial_input = input("ì´ˆê¸° ìŠ¤íƒ€íŠ¸ì—… ì •ë³´ë¥¼ ê°„ëµíˆ ì…ë ¥í•˜ì„¸ìš”: ")
    # ì‹ë³„ì: USER_INPUT (ìµœì´ˆ ì…ë ¥)
    collected_contexts.append(f"USER_INPUT: {initial_input}")

    # 2) ì²« ë³´ê³ ì„œ ìƒì„±
    final_markdown = generate_business_report(report_card, collected_contexts)
    print("[ì´ˆê¸° ë³´ê³ ì„œ]\n", final_markdown)

    while iteration_count < max_iterations:
        iteration_count += 1
        print(f"\n=== [Iteration {iteration_count}] ===")
        print_report(report_card)

        # (A) ëª¨ë“  í•­ëª©ì´ threshold ì´ìƒì´ë©´ ì¢…ë£Œ
        if all_criteria_above_threshold(report_card, CONFIDENCE_THRESHOLD):
            print("âœ… ëª¨ë“  í•­ëª©ì´ thresholdë¥¼ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ìµœì¢… ë³´ê³ ì„œë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.")
            break

        # (B) LLMì—ê²Œ â€œë‹¤ìŒ ì•¡ì…˜â€ ì§ˆì˜
        action_data = ask_llm_for_next_action(report_card, collected_contexts, action_history)
        if action_data is None:
            print("âŒ 3íšŒ ì‹œë„ í›„ì—ë„ ì•¡ì…˜ JSON íŒŒì‹± ì‹¤íŒ¨. ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        chosen_criterion = action_data["criterion"]
        chosen_action = action_data["action"]
        rationale = action_data["rationale"]

        print(f"[LLM ê²°ì •] ë‹¤ìŒì— ì§‘ì¤‘í•  í•­ëª©: {chosen_criterion}")
        print(f"[LLM ê²°ì •] ì„ íƒëœ ì•¡ì…˜: {chosen_action}")
        print(f"[LLM ê²°ì •] ì‚¬ìœ (rationale): {rationale}")

        if chosen_action == "NoActionNeeded":
            print("LLMì´ NoActionNeededë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤. ë³´ê³ ì„œë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break

        # (C) ì•¡ì…˜ ìˆ˜í–‰ â†’ ìƒˆ ì •ë³´ íšë“
        # if iteration_count == 1:
        #     chosen_action = "AnalyzeAndVisualize"
        action_result = perform_action(chosen_action, chosen_criterion, collected_contexts)
        print(f"[Action ê²°ê³¼] {action_result}")

        # ì•¡ì…˜ ì´ë ¥ì— ì¶”ê°€
        action_log_text = (
            f"Iteration={iteration_count}, "
            f"Action={chosen_action}, "
            f"Criterion={chosen_criterion}, "
            f"Rationale={rationale}, "
            f"Result={action_result}"
        )
        action_history.append(action_log_text)

        # ê²°ê³¼ë¥¼ contextì—ë„ ì¶”ê°€
        collected_contexts.append(action_result)

        # (D) LLMìœ¼ë¡œ ë³´ê³ ì„œ ì¬ì‘ì„±
        final_markdown = generate_business_report(report_card, collected_contexts)
        print("[LLM ìµœì¢… ë³´ê³ ì„œ(ë§ˆí¬ë‹¤ìš´)]\n", final_markdown)

    # ë°˜ë³µ ì¢…ë£Œ ì‹œì , ìµœì¢… ì¶œë ¥
    print("\n=== ìµœì¢… ìŠ¤íƒ€íŠ¸ì—… ë³´ê³ ì„œ ===")
    print_report(report_card)
    print("ë³´ê³ ì„œ ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. âœ…")

    # (F) ëª¨ë“  ì‘ì—… í›„, discussion_loop() ì§„ì… (ìƒˆ ê¸°ëŠ¥)
    all_user_inputs = []
    for ctx in collected_contexts:
        if ctx.startswith("USER_INPUT:"): 
            all_user_inputs.append(ctx[len("USER_INPUT:"):].strip())
    discussion_loop(final_markdown, all_user_inputs)
